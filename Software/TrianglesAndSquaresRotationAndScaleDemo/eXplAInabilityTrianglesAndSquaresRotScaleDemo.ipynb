{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Explainability Demo\n",
    "\n",
    "*At the moment this is work in progress and in the experimental phase. If the experiment is successsful, e.g. we get meaningful heatmaps for the task, this will turn into a real demo!*\n",
    "\n",
    "**The purpose of this demo is to show the ability of DNN explainability method(s) to generate meanfingful heatmaps for the relevance of the input image pixels to the output resut of a CNN model. The task is chosen to be lassification of 2 simple shapes - triangle and square rather than a classical object classificaiton task to minimize human interpretation of the resulting heatmaps.**\n",
    "\n",
    "The explainability tool [iNNvestigate](https://github.com/albermax/innvestigate) is chosen as it offers open source implementatons of several explainability methods. Since it supports [Keras](https://keras.io/), the framework (with [Tensorflow](https://www.tensorflow.org/) as backend) is used to genrate the CNN model(s), though our vision is to go for at least [PyTorch](https://pytorch.org/) deep learning library and the related [Captum](https://captum.ai/) explainability library and definitely towards using DNN model standards such as [ONNX](https://onnx.ai/). This [list of explainability tools](https://github.com/NLeSC/XAI/blob/master/State-of-the-art/AI_Explainability_OSTools.docx) sumamrizes the availability of explainability tools.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "The original dataset has been generated in MATLAB and stored at [SURF drive](https://surfdrive.surf.nl/files/index.php/s/MoCVal7gxS4aX51?path=%2Fdata%2FTrianglesAndSquaresRotationScale). There are 20 000 gray scale images with either (rotated and scaled) triangle or square o\n",
    "\n",
    "The notebook [DataPreparation](https://github.com/NLeSC/XAI/blob/master/Software/TrianglesAndSquaresRotationAndScaleDemo/DataPreparation.ipynb) is used to load the original data, split the dataset into training, testing and validation and save the files as numpy compressed (NPZ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "The generated data per dataset have been split in ... for training, ... for validaiton and ... for testing. \n",
    "The link to the notebook for the model training:\n",
    "\n",
    "The obtained accuracies are as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing\n",
    "\n",
    "The link to the notebook for the model testing:\n",
    "\n",
    "\n",
    "\n",
    "The obtained test accuracy: %\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability heatmaps\n",
    "\n",
    "The links to the notebooks for generating explainability heatmaps:\n",
    "\n",
    "\n",
    "-----------------------\n",
    "Experiments\n",
    "\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
