{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Several analysers of the classification model on many images of the  on Triangles and Squares (Rotaion & Scaling) dataset using iNNvestigate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "if(K.tensorflow_backend):\n",
    "    import tensorflow as tf\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "mnistutils = imp.load_source(\"utils_mnist\", \"/home/elena/eStep/XAI/Software/innvestigate/examples/utils_mnist.py\")\n",
    "eutils = imp.load_source(\"utils\", \"/home/elena/eStep/XAI/Software/innvestigate/examples/utils.py\")\n",
    "\n",
    "from code import shape_images as shi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames\n",
    "# data paths\n",
    "original_data_path = \"/home/elena/eStep/XAI/Data/TrianglesAndSquaresRotationScale\"\n",
    "\n",
    "train_data_fname = os.path.join(original_data_path, 'split_npz','train_data.npz')\n",
    "test_data_fname = os.path.join(original_data_path, 'split_npz','test_data.npz')\n",
    "val_data_fname = os.path.join(original_data_path, 'split_npz','validation_data.npz')\n",
    "\n",
    "# loading\n",
    "train_data = np.load(train_data_fname)\n",
    "test_data = np.load(test_data_fname)\n",
    "val_data = np.load(val_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = train_data['images_train']\n",
    "labels_train = train_data['labels_train']\n",
    "images_test = test_data['images_test']\n",
    "labels_test_or = test_data['labels_test']\n",
    "images_val = val_data['images_val']\n",
    "labels_val = val_data['labels_val']\n",
    "\n",
    "print(\"Size of training data: \", np.shape(images_train), \"and labels: \", np.shape(labels_train))\n",
    "print(\"Size of validation data: \", np.shape(images_val), \"and labels: \", np.shape(labels_val))\n",
    "print(\"Size of testing data: \", np.shape(images_test), \"and labels: \", np.shape(labels_test_or))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 64\n",
    "img_cols = 64\n",
    "#print(K.image_data_format())\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    images_train = images_train.reshape(images_train.shape[0], 1, img_rows, img_cols)\n",
    "    images_test = images_test.reshape(images_test.shape[0], 1, img_rows, img_cols)\n",
    "    images_val = images_val.reshape(images_val.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    images_train = images_train.reshape(images_train.shape[0], img_rows, img_cols, 1)\n",
    "    images_test = images_test.reshape(images_test.shape[0], img_rows, img_cols, 1)\n",
    "    images_val = images_val.reshape(images_val.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "print(\"Size of training data: \", np.shape(images_train))\n",
    "print(\"Size of validation data: \", np.shape(images_val))\n",
    "print(\"Size of testing data: \", np.shape(images_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 12 random train images\n",
    "shi.plot_12images(images_train, labels_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename for model \n",
    "model_fname = os.path.join(original_data_path, 'Models','model.h5')\n",
    "\n",
    "# load the trained model\n",
    "model = load_model(model_fname) \n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(images_test, labels_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1]*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up 10 random images and classify them using the trained model\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "figsize = (8, 6)\n",
    "plt.figure(figsize=figsize)\n",
    "j=0\n",
    "nim = len(labels_test_or)\n",
    "\n",
    "for _ in range(10):\n",
    "    ind=int(np.random.randint(1,nim))\n",
    "    img=images_test[ind,:]\n",
    "    img=np.reshape(img,(64,64))\n",
    "    X = img[np.newaxis, :, :, np.newaxis].astype(np.float32)\n",
    "    label=labels_test_or[ind]\n",
    "       \n",
    "    predictions = model.predict(X);\n",
    "    #print(predictions)\n",
    "    pred = np.argmax(predictions) \n",
    "    #print(pred)\n",
    "    \n",
    "    j = j+1\n",
    "    plt.subplot(2, 5, j)\n",
    "    plt.imshow(img*255,cmap=cm.gray, vmin=0, vmax=255)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('n=%d n̂=%d' % (label, pred))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the indicies of the missclassified images and display some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(images_test)\n",
    "indices_wrong = [i for i,v in enumerate(predictions) if np.argmax(predictions[i])!=np.argmax(labels_test[i])]\n",
    "print('Missclassified images: ', len(indices_wrong)/len(labels_test)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 10 missclassified images \n",
    "\n",
    "for i in range(10):\n",
    "      \n",
    "    ind = indices_wrong[i]\n",
    "    img = images_test[ind].reshape(img_rows, img_cols)\n",
    "    lab = np.argmax(labels_test[ind])+1 # we subtracted 1 before\n",
    "    pred = np.argmax(predictions[ind])+1\n",
    "    \n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(img,cmap='binary')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([]) \n",
    "    plt.title('n=%d n̂=%d' % (lab, pred))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of top analysis methods by preparing tuples containing the methods' string identifiers used by innvestigate.analyzer.create_analyzer(...), some optional parameters, a post processing choice for visualizing the computed analysis and a title for the figure to render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "input_range = [0, 1]\n",
    "preprocess, revert_preprocessing = mnistutils.create_preprocessing_f(images_test, input_range)\n",
    "def input_postprocessing(X):\n",
    "    #return revert_preprocessing(X) / 255\n",
    "    return 1-X\n",
    "\n",
    "\n",
    "noise_scale = (input_range[1]-input_range[0]) * 0.1\n",
    "ri = input_range[0]  # reference input\n",
    "\n",
    "# Configure analysis methods and properties\n",
    "methods = [\n",
    "    # NAME            OPT.PARAMS                POSTPROC FXN               TITLE\n",
    "\n",
    "    # Show input\n",
    "    (\"input\",         {},                       input_postprocessing,      \"Input\"),\n",
    "\n",
    "    # Signal\n",
    "    (\"pattern.net\",   {\"pattern_type\": \"relu\"}, mnistutils.bk_proj,        \"PatternNet\"), \n",
    "\n",
    "\n",
    "    # Interaction\n",
    "    (\"pattern.attribution\",  {\"pattern_type\": \"relu\"}, mnistutils.heatmap,        \"PatternAttribution\"),\n",
    "       \n",
    "    (\"deep_taylor.bounded\",   {\"low\": input_range[0],\"high\": input_range[1]}, mnistutils.heatmap,        \"DeepTaylor\"),\n",
    "    \n",
    "    (\"input_t_gradient\",     {},                        mnistutils.heatmap,        \"Input * Gradient\"),\n",
    "    \n",
    "    (\"integrated_gradients\", {\"reference_inputs\": ri},  mnistutils.heatmap,        \"Integrated Gradients\"), \n",
    "\n",
    "    (\"lrp.z\",                {},                        mnistutils.heatmap,        \"LRP-Z\"),\n",
    "]\n",
    "    \n",
    "print('Considered number of explainability methods:', len(methods)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop instantiates the analyzer objects based on the loaded/trained model and the analyzers' parameterizations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model without trailing softmax\n",
    "model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)\n",
    "\n",
    "path_to_analyzers = \"/home/elena/eStep/XAI/Data/TrianglesAndSquaresRotationScale/Analyzers/\"  \n",
    "\n",
    "\n",
    "# Create analyzers.\n",
    "analyzers = []\n",
    "for method in methods:\n",
    "    fname = os.path.join(path_to_analyzers, method[0]+'.npz')\n",
    "    \n",
    "    analyzer = innvestigate.create_analyzer(method[0],        # analysis method identifier\n",
    "                                            model_wo_softmax, # model without softmax output\n",
    "                                            neuron_selection_mode=\"index\",\n",
    "                                            **method[1])     \n",
    "    \n",
    "    if os.path.isfile(fname) :\n",
    "        print(\"Analyzer\", method[0], \"exists! Loading...\")\n",
    "        analyzer = analyzer.load_npz(fname)\n",
    "    else:\n",
    "        print(\"Analyzer\", method[0], \" doesn't exist! Creating and possibly Training and [Saving]...\")\n",
    "        # Some analyzers require training.\n",
    "        analyzer.fit(images_train, batch_size=256, verbose=1)\n",
    "        if (method[0]=='pattern.net') or (method[0]=='pattern.attribution'):\n",
    "            analyzer.save_npz(fname)\n",
    "    \n",
    "    analyzers.append(analyzer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze random set of test images with the different analyzers on all output neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nim = len(labels_test)\n",
    "ntim = 25\n",
    "ind = int(np.random.randint(1,nim-ntim))\n",
    "\n",
    "test_images = list(zip(images_test[ind:ind+ntim], numerical_labels_test[ind:ind+ntim]-1))\n",
    "label_to_class_name = [str(i+1) for i in range(num_classes)]\n",
    "\n",
    "for image_nr, (x, y) in enumerate(test_images):\n",
    "    # Add batch axis.\n",
    "    x = x[None, :, :, :]\n",
    "\n",
    "    analysis = np.zeros([num_classes, len(analyzers), 64, 64, 3])\n",
    "    text = []\n",
    "\n",
    "    for ii, output_neuron in enumerate([0, 1, 2]):\n",
    "        # Predict final activations, probabilites, and label.\n",
    "        presm = model_wo_softmax.predict_on_batch(x)[0]\n",
    "        prob = model.predict_on_batch(x)[0]\n",
    "        y_hat = prob.argmax()\n",
    "\n",
    "        # Save prediction info:\n",
    "        text.append((\"%s\" % label_to_class_name[y],    # ground truth label\n",
    "                     \"%.2f\" % presm[output_neuron],    # pre-softmax logits\n",
    "                     \"%.2f\" % prob[output_neuron],     # probabilistic softmax output  \n",
    "                     \"%s\" % label_to_class_name[output_neuron]\n",
    "                    ))\n",
    "       \n",
    "\n",
    "        for aidx, analyzer in enumerate(analyzers):\n",
    "            # Analyze.\n",
    "            a = analyzer.analyze(x, neuron_selection=output_neuron)\n",
    "\n",
    "            # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
    "            a = mnistutils.postprocess(a)\n",
    "            # Apply analysis postprocessing, e.g., creating a heatmap.\n",
    "            a = methods[aidx][2](a)\n",
    "            # Store the analysis.\n",
    "            analysis[ii, aidx] = a[0]\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    print(\"Image nr. {}; prediction: {} \".format(ind+image_nr, y_hat+1))\n",
    "    # Prepare the grid as rectengular list\n",
    "    grid = [[analysis[i, j] for j in range(analysis.shape[1])]\n",
    "            for i in range(analysis.shape[0])]\n",
    "    # Prepare the labels\n",
    "    label, presm, prob, pred = zip(*text)\n",
    "    print(label)\n",
    "    row_labels_left = [('label: {}'.format(label[i]), 'neuron: {}'.format(pred[i])) for i in range(len(label))]\n",
    "   # row_labels_right = []\n",
    "    row_labels_right = [('logit: {}'.format(presm[i]), 'prob: {}'.format(prob[i])) for i in range(len(label))]\n",
    "    col_labels = [''.join(method[3]) for method in methods]\n",
    "\n",
    "    # Plot the analysis.\n",
    "    file_name = os.environ.get(\"PLOTFILENAME\", None)\n",
    "   # print(file_name)\n",
    "    if file_name is not None:\n",
    "        file_name = \".\".join(file_name.split(\".\")[:-1])+(\"_%i\" % output_neuron)+file_name.split(\".\")[-1]   \n",
    "    n_rows = len(grid)\n",
    "    n_cols = len(grid[0])\n",
    "    figsize = (2*n_cols, 2*(n_rows+1))\n",
    "    eutils.plot_image_grid(grid, row_labels_left, row_labels_right, col_labels, figsize=figsize, file_name=file_name)nim = len(labels_test)\n",
    "ntim = 25\n",
    "ind = int(np.random.randint(1,nim-ntim))\n",
    "\n",
    "test_images = list(zip(images_test[ind:ind+ntim], numerical_labels_test[ind:ind+ntim]-1))\n",
    "label_to_class_name = [str(i+1) for i in range(num_classes)]\n",
    "\n",
    "for image_nr, (x, y) in enumerate(test_images):\n",
    "    # Add batch axis.\n",
    "    x = x[None, :, :, :]\n",
    "\n",
    "    analysis = np.zeros([num_classes, len(analyzers), 64, 64, 3])\n",
    "    text = []\n",
    "\n",
    "    for ii, output_neuron in enumerate([0, 1, 2]):\n",
    "        # Predict final activations, probabilites, and label.\n",
    "        presm = model_wo_softmax.predict_on_batch(x)[0]\n",
    "        prob = model.predict_on_batch(x)[0]\n",
    "        y_hat = prob.argmax()\n",
    "\n",
    "        # Save prediction info:\n",
    "        text.append((\"%s\" % label_to_class_name[y],    # ground truth label\n",
    "                     \"%.2f\" % presm[output_neuron],    # pre-softmax logits\n",
    "                     \"%.2f\" % prob[output_neuron],     # probabilistic softmax output  \n",
    "                     \"%s\" % label_to_class_name[output_neuron]\n",
    "                    ))\n",
    "       \n",
    "\n",
    "        for aidx, analyzer in enumerate(analyzers):\n",
    "            # Analyze.\n",
    "            a = analyzer.analyze(x, neuron_selection=output_neuron)\n",
    "\n",
    "            # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
    "            a = mnistutils.postprocess(a)\n",
    "            # Apply analysis postprocessing, e.g., creating a heatmap.\n",
    "            a = methods[aidx][2](a)\n",
    "            # Store the analysis.\n",
    "            analysis[ii, aidx] = a[0]\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    print(\"Image nr. {}; prediction: {} \".format(ind+image_nr, y_hat+1))\n",
    "    # Prepare the grid as rectengular list\n",
    "    grid = [[analysis[i, j] for j in range(analysis.shape[1])]\n",
    "            for i in range(analysis.shape[0])]\n",
    "    # Prepare the labels\n",
    "    label, presm, prob, pred = zip(*text)\n",
    "    print(label)\n",
    "    row_labels_left = [('label: {}'.format(label[i]), 'neuron: {}'.format(pred[i])) for i in range(len(label))]\n",
    "   # row_labels_right = []\n",
    "    row_labels_right = [('logit: {}'.format(presm[i]), 'prob: {}'.format(prob[i])) for i in range(len(label))]\n",
    "    col_labels = [''.join(method[3]) for method in methods]\n",
    "\n",
    "    # Plot the analysis.\n",
    "    file_name = os.environ.get(\"PLOTFILENAME\", None)\n",
    "   # print(file_name)\n",
    "    if file_name is not None:\n",
    "        file_name = \".\".join(file_name.split(\".\")[:-1])+(\"_%i\" % output_neuron)+file_name.split(\".\")[-1]   \n",
    "    n_rows = len(grid)\n",
    "    n_cols = len(grid[0])\n",
    "    figsize = (2*n_cols, 2*(n_rows+1))\n",
    "    eutils.plot_image_grid(grid, row_labels_left, row_labels_right, col_labels, figsize=figsize, file_name=file_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
